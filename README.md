# InfroTheory_project

In this project we will discuss the Divergence types, namly KL Divergence and Jensen-Shannon Divergence
and try to explain with example how we can use them to measure similarity or difference between the distribuations.

We will also Implement the minimizing of KL Divergence for Normal Distribuation using Gradient Descent 
in Tensorflows and plot the results of training and showing how to achiving minimum KL divergence.

We will aslo show the ussage of KL Divergence in Neural networks training models and how we can 
acheive better training and better output results in Neural Networks.
